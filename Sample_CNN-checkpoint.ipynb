{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils import data # 获取迭代数据\n",
    "from torch.autograd import Variable # 获取变量\n",
    "import torchvision\n",
    "from torchvision.datasets import mnist # 获取数据集\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取数据\n",
    "data_tf = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.5],[0.5])\n",
    "    ]\n",
    ")\n",
    "\n",
    "data_path = r'E:\\Python\\jupyter_notebook\\GCN'\n",
    "# 获取数据集\n",
    "train_data = mnist.MNIST(data_path,train=True,transform=data_tf,download=True)\n",
    "test_data = mnist.MNIST(data_path,train=False,transform=data_tf,download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取迭代数据\n",
    "# batch_size:how many samples per batch to load\n",
    "# set to True to have the data reshuffled at every epoch\n",
    "train_loader = data.DataLoader(train_data,batch_size=128,shuffle=True)\n",
    "test_loader = data.DataLoader(test_data,batch_size=100,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x00000273A5715BB0>\n"
     ]
    }
   ],
   "source": [
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1、数据格式类型\n",
    "### 其中1，16，32，64都是通道数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jupyter](./Data_Type.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2、定义网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNnet(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (conv4): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (mlp1): Linear(in_features=256, out_features=100, bias=True)\n",
      "  (mlp2): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#定义网络\n",
    "class CNNnet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNnet,self).__init__()\n",
    "        \n",
    "        self.conv1=torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=16,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=1\n",
    "        ),\n",
    "        #对数据进行归一化\n",
    "        torch.nn.BatchNorm2d(16),\n",
    "        torch.nn.ReLU()\n",
    "        )\n",
    "        self.conv2=torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(16,32,3,2,1),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.conv3=torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32,64,3,2,1),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.conv4=torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(64,64,2,2,0),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.mlp1=torch.nn.Linear(2*2*64,100)\n",
    "        self.mlp2=torch.nn.Linear(100,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.conv1(x)\n",
    "        x=self.conv2(x)\n",
    "        x=self.conv3(x)\n",
    "        x=self.conv4(x)\n",
    "        x=self.mlp1(x.view(x.size(0),-1))\n",
    "        x=self.mlp2(x)\n",
    "        return x\n",
    "    \n",
    "model = CNNnet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用交叉熵损失\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "#使用Adm优化器\n",
    "opt = torch.optim.Adam(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model.parmaters()含义：\n",
    "#### 使用损失和优化器的步骤：\n",
    "#### 获取损失：loss = loss_func(out,batch_y)\n",
    "#### 清空上一步残余更新参数：opt.zero_grad()\n",
    "#### 误差反向传播：loss.backward()\n",
    "#### 将参数更新值施加到net的parmeters上：opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3、训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\t 0.03334479779005051\n",
      "accuray:\t 0.97\n",
      "20:\t 0.051606982946395874\n",
      "40:\t 0.01883035898208618\n",
      "60:\t 0.08852490037679672\n",
      "80:\t 0.006442378740757704\n",
      "100:\t 0.006954974960535765\n",
      "accuray:\t 1.0\n",
      "120:\t 0.040239445865154266\n",
      "140:\t 0.09907270967960358\n",
      "160:\t 0.04330354928970337\n",
      "180:\t 0.08192750811576843\n",
      "200:\t 0.08558496087789536\n",
      "accuray:\t 0.99\n",
      "220:\t 0.021817712113261223\n",
      "240:\t 0.048462290316820145\n",
      "260:\t 0.02016281522810459\n",
      "280:\t 0.006021992303431034\n",
      "300:\t 0.05197949707508087\n",
      "accuray:\t 0.99\n",
      "320:\t 0.042473260313272476\n",
      "340:\t 0.061597343534231186\n",
      "360:\t 0.14985011518001556\n",
      "380:\t 0.126044362783432\n",
      "400:\t 0.05092267692089081\n",
      "accuray:\t 0.97\n",
      "420:\t 0.04366695135831833\n",
      "440:\t 0.06237776204943657\n",
      "460:\t 0.01492332760244608\n",
      "0:\t 0.05027630925178528\n",
      "accuray:\t 0.97\n",
      "20:\t 0.026027631014585495\n",
      "40:\t 0.040824830532073975\n",
      "60:\t 0.009312434121966362\n",
      "80:\t 0.04192877933382988\n",
      "100:\t 0.042520053684711456\n",
      "accuray:\t 0.98\n",
      "120:\t 0.10510491579771042\n",
      "140:\t 0.024082431569695473\n",
      "160:\t 0.041052743792533875\n",
      "180:\t 0.006999895442277193\n",
      "200:\t 0.02259550243616104\n",
      "accuray:\t 0.99\n",
      "220:\t 0.018980473279953003\n",
      "240:\t 0.029694296419620514\n",
      "260:\t 0.009296162985265255\n",
      "280:\t 0.02346523106098175\n",
      "300:\t 0.03609137609601021\n",
      "accuray:\t 0.99\n",
      "320:\t 0.04285311698913574\n",
      "340:\t 0.010317319072782993\n",
      "360:\t 0.0393240824341774\n",
      "380:\t 0.018992936238646507\n",
      "400:\t 0.0383373387157917\n",
      "accuray:\t 0.98\n",
      "420:\t 0.01408845279365778\n",
      "440:\t 0.04808792844414711\n",
      "460:\t 0.04086798429489136\n"
     ]
    }
   ],
   "source": [
    "loss_count=[]\n",
    "for epoch in range(2):\n",
    "    for i,(x,y) in enumerate(train_loader):\n",
    "        batch_x=Variable(x) #torch.Size([128, 1, 28, 28])\n",
    "        batch_y=Variable(y) #torch.Size([128])\n",
    "        #获取最后的输出，out为训练得到输出的值，batch_y为实际输出的值\n",
    "        out=model(batch_x)\n",
    "        #获取损失\n",
    "        loss = loss_func(out,batch_y)\n",
    "        #使用优化器优化损失\n",
    "        opt.zero_grad()#清空上一步残余更新参数值\n",
    "        loss.backward() # 误差反向传播，计算参数更新值\n",
    "        opt.step() # 将参数更新值施加到net的parmeters上\n",
    "        if i%20==0:\n",
    "            loss_count.append(loss)\n",
    "            print('{}:\\t'.format(i),loss.item())\n",
    "            torch.save(model,'save.pt')\n",
    "        if i%100==0:\n",
    "            for a,b in test_loader:\n",
    "                test_x=Variable(a)\n",
    "                test_y=Variable(b)\n",
    "                out=model(test_x)\n",
    "                # print('test_out:\\t',torch.max(out,1)[1])\n",
    "                # print('test_y:\\t',test_y)\n",
    "                accuray=torch.max(out,1)[1].numpy()==test_y.numpy()\n",
    "                print('accuray:\\t',accuray.mean())\n",
    "                break\n",
    "        #print(loss_count)\n",
    "         \n",
    "#plt.figure('PyTorch_CNN_Loss')\n",
    "plt.plot(loss_count,lable='loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-87404ffbb8f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'loss_count' is not defined"
     ]
    }
   ],
   "source": [
    "print(loss_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch基础入门教程/一小时学会pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、张量的定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.3674e-39, 9.9184e-39, 8.7245e-39],\n",
      "        [9.2755e-39, 8.9082e-39, 9.9184e-39],\n",
      "        [8.4490e-39, 9.6429e-39, 1.0653e-38],\n",
      "        [1.0469e-38, 4.2246e-39, 1.0378e-38],\n",
      "        [9.6429e-39, 9.2755e-39, 9.7346e-39]])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "x=torch.empty(5,3)#创建一个未初始化的张量\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2407, 0.1311, 0.6490],\n",
      "        [0.7976, 0.1534, 0.4332],\n",
      "        [0.8629, 0.9067, 0.5806],\n",
      "        [0.1761, 0.5129, 0.5723],\n",
      "        [0.4268, 0.5230, 0.3272]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)#创建一个初始化的张量，每个元素0~1变化。\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5.5, 3])#将已有的矩阵转化为张量\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = x.new_ones(5, 3, dtype=torch.double)#从已有的张量中创造一个张量\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())#获取张量的形状"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、张量的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5767, 1.5550, 1.5191],\n",
      "        [1.3939, 1.7041, 1.8162],\n",
      "        [1.5201, 1.6186, 1.1870],\n",
      "        [1.1851, 1.1572, 1.9509],\n",
      "        [1.3536, 1.1982, 1.6559]], dtype=torch.float64)\n",
      "tensor([[1.5767, 1.5550, 1.5191],\n",
      "        [1.3939, 1.7041, 1.8162],\n",
      "        [1.5201, 1.6186, 1.1870],\n",
      "        [1.1851, 1.1572, 1.9509],\n",
      "        [1.3536, 1.1982, 1.6559]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "y=torch.rand(5,3)\n",
    "#加法\n",
    "print(y+x)\n",
    "print(torch.add(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5767, 1.5550, 1.5191],\n",
      "        [1.3939, 1.7041, 1.8162],\n",
      "        [1.5201, 1.6186, 1.1870],\n",
      "        [1.1851, 1.1572, 1.9509],\n",
      "        [1.3536, 1.1982, 1.6559]])\n",
      "tensor([[1.5767, 1.5550, 1.5191],\n",
      "        [1.3939, 1.7041, 1.8162],\n",
      "        [1.5201, 1.6186, 1.1870],\n",
      "        [1.1851, 1.1572, 1.9509],\n",
      "        [1.3536, 1.1982, 1.6559]])\n"
     ]
    }
   ],
   "source": [
    "#将x+y的值传给result\n",
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)\n",
    "#将x+y的值传给y\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6345,  1.1019,  0.0431,  0.0237],\n",
      "        [ 2.0844,  0.2365,  0.5201,  2.0613],\n",
      "        [ 0.2422, -0.3520, -0.5382,  0.7391],\n",
      "        [-0.5221, -1.6532, -1.5668,  3.1315]])\n",
      "torch.Size([4, 4])\n",
      "tensor([ 0.6345,  1.1019,  0.0431,  0.0237,  2.0844,  0.2365,  0.5201,  2.0613,\n",
      "         0.2422, -0.3520, -0.5382,  0.7391, -0.5221, -1.6532, -1.5668,  3.1315])\n",
      "tensor([[ 0.6345,  1.1019,  0.0431,  0.0237,  2.0844,  0.2365,  0.5201,  2.0613],\n",
      "        [ 0.2422, -0.3520, -0.5382,  0.7391, -0.5221, -1.6532, -1.5668,  3.1315]])\n"
     ]
    }
   ],
   "source": [
    "#调整张量的形状\n",
    "x = torch.randn(4, 4)\n",
    "print(x),print(x.size())\n",
    "y = x.view(16)\n",
    "print(y)\n",
    "z = x.view(-1, 8)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、张量和Numpy的相互转化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "#将张量a转化为numpy b\n",
    "b=a.numpy()\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#将numpy a转化为张量b,当a的值变化时，b的值也会变化\n",
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4、自动微分\n",
    "#### 4.1、张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x00000177818DCB50>\n",
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n",
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x00000177818DCB50>\n"
     ]
    }
   ],
   "source": [
    "#设置一个张量x。\n",
    "# requires_grad：设置为Trued的时候，它会开始追踪这个张量上的运算，求导的时候会对这个变量自动求导\n",
    "# grad_fn：用于指导反向传播，知道变量是怎么来的\n",
    "x=torch.ones(2,2,requires_grad=True)\n",
    "y=x+2\n",
    "z=y*y*3\n",
    "out=z.mean()\n",
    "print(x)\n",
    "print(y)\n",
    "print(y.grad_fn)\n",
    "print(z,out)\n",
    "\n",
    "a=torch.randn(2,3)\n",
    "a=((a*3)/(a-1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True) #设置requires_grad的公式\n",
    "print(a.requires_grad)\n",
    "b=(a*a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2、 梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1=torch.tensor(2.0,requires_grad=True)\n",
    "a=torch.tensor([[1.,2.],[3.,4.]],requires_grad=True)\n",
    "tmp=a[0,:]\n",
    "tmp.retain_grad() #tmp是非叶子张量，所以在运算过程中需要通过retain_grad()来保留导数\n",
    "b=tmp.repeat([4,1])\n",
    "b.retain_grad()\n",
    "loss=(b*w1).mean()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2., requires_grad=True)\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]], requires_grad=True)\n",
      "tensor([1., 2.], grad_fn=<SliceBackward>)\n",
      "tensor([[1., 2.],\n",
      "        [1., 2.],\n",
      "        [1., 2.],\n",
      "        [1., 2.]], grad_fn=<RepeatBackward>)\n",
      "tensor(3., grad_fn=<MeanBackward0>)\n",
      "tensor([[0.2500, 0.2500],\n",
      "        [0.2500, 0.2500],\n",
      "        [0.2500, 0.2500],\n",
      "        [0.2500, 0.2500]])\n",
      "tensor(1.5000)\n",
      "tensor([1., 1.])\n"
     ]
    }
   ],
   "source": [
    "print(w1)\n",
    "print(a)\n",
    "print(tmp)\n",
    "print(b)\n",
    "print(loss)\n",
    "print(b.grad)\n",
    "print(w1.grad)\n",
    "print(tmp.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
